from collections import OrderedDict

class twtokenizer():
    
    def __init__(self):
        self.toReplaceDict = OrderedDict({'!!*':' ! ','\?':' ? ', '\"':' " ',"â€œ":" â€œ ","â€":" â€ ", "\'\'*":"'","\' ":" ' "
    ," \'":" ' ","â€™ ":" â€™ ",'&amp;':'&','&gt;':'>','&lt;':'<', '~~*':' ~ ',"Â¿Â¿*":" Â¿ ",'\.\.\.':' ... ','\.\.':' .. '
    ,'â€¦':' â€¦ ',"\(\(*":'(',"\)\)*":')',"\+\+*":'+',"\*\**":'*',"\|\|*":"|","\$\$*":"$","%%*":"%",">>*":">","<<*":"<","--*":"-" 
    ,"\/\/\/*":"//","(:d)(:d)*":":d",":ddd*":" :d ",":ppp*":" :p ",";;;*":";",":\* ":" :* ",":\(":" :( ","\(:":" (: ",":\)":" :) "
    ,'\):':' ): ',";\)":" ;) ","\+\+":" + ",":\|":" :| ",":-\)":" :-) ",";-\)":" ;-) ",":-\(":" :-( ",":\'\(":" :'( ",":p ":" :p "
    ,";p ":" ;p ",":d ":" :d ","-_-":" -_- ",":o\)":" :o) ",":\$":" :$ ","\.@":". @",'#':' #','\. ': ' . ',' \.': ' . ','    ':' '
    ,'   ':' ','   ':' ','  ':' ',"ðŸ˜¡ðŸ˜¡*":" :( ","â˜ºï¸â˜ºï¸*":" :) ","ðŸ˜„ðŸ˜„*":" :d ","ðŸ˜ƒðŸ˜ƒ*":" :d ","ðŸ˜†ðŸ˜†*":" :d ","ðŸ˜·ðŸ˜·*":" :d "
    ,"ðŸ˜…ðŸ˜…*":" :d ","ðŸ˜‹ðŸ˜‹*":" :d ","ðŸ˜œðŸ˜œ*":" :p " ,"ðŸ˜ðŸ˜*":" :p ","ðŸ˜‚ðŸ˜‚*":" :'( ","ðŸ˜¢ðŸ˜¢*":" :'( ","ðŸ˜ðŸ˜*":" :( ","ðŸ˜žðŸ˜ž*":" :( "
    ,"ðŸ˜–ðŸ˜–*":" :( " ,"ðŸ˜¥ðŸ˜¥*":" :( ","ðŸ˜©ðŸ˜©*":" :( ","ðŸ˜ŠðŸ˜Š*":" :) ","ðŸ˜‰ðŸ˜‰*":" :) ","ðŸ˜ŽðŸ˜Ž*":" :) " ,"ðŸ˜‡ðŸ˜‡*":" :) ","ðŸ˜­ðŸ˜­*":" :'d " 
    ,"ðŸ˜¨ðŸ˜¨*":" :| ","ðŸ˜ðŸ˜*":" :| " ,"ðŸ˜”ðŸ˜”*":" :| ","ðŸ˜’ðŸ˜’*":" :| ","ðŸ˜«ðŸ˜«*":" :( ","ðŸ˜ªðŸ˜ª*":" :'( "
    ,"ðŸ˜°ðŸ˜°*":" :'( " ,"ðŸ˜ðŸ˜*":" <3 ","ðŸ˜˜ðŸ˜˜*":" <3 ","<33*":" <3 ","<3(<3)*":" <3 ","ðŸ˜³ðŸ˜³*":" ðŸ˜³ ", "ðŸ˜»ðŸ˜»*":" ðŸ˜» ", "\n\n*":" \n ", "â™ªâ™ª*":" â™ª "
    ,"ðŸ’§ðŸ’§*":" ðŸ’§ ", """\xa0""":" ", "\n":" . ","ã€ã€*":" ã€ ","ã€‘ã€‘*":" ã€‘ ","ã€Œã€Œ*":" ã€Œ ","ã€ã€*":" ã€ ","â¤ï¸â¤ï¸*":" <3 ","ðŸŽ¶ðŸŽ¶*":" ðŸŽ¶ "
    ,"ðŸ˜ŒðŸ˜Œ*":" :) ","ðŸ’–ðŸ’–*":" <3 ","ðŸ˜ðŸ˜*":" :| ","\.: ":" .: "})

    def tokenize(self, tw):
        newtw = ''
        lentw = len(tw)
        for i, c in enumerate(tw):
            if (c in "'`Â´â€™â€˜") and ((i+1 != lentw) and (i!=0)) and ((tw[i-1].isalpha()) or tw[i-1] in "0123456789") and (tw[i+1].isalpha()):
                newtw += ' '+c+' '
            elif (c in "'`Â´â€™()+*-") and ((lentw>i+1) and (i>0)) and (tw[i-1] in ' 0123456789') and (tw[i+1].isalpha()):
                newtw += c+' '
            elif (c in '();>:.') and ((lentw>i+1) and (i!=0)) and ((tw[i-1].isalpha()) or (tw[i-1] in '0123456789')) and ((tw[i+1] == ' ') or (i == lentw-1)):
                newtw += ' '+c
            elif (c in "'`Â´â€™â€˜()+*->") and (i==0) and (lentw > 1) and ((tw[1].isalpha()) or tw[1] in "0123456789"):
                newtw += c+' '
            elif (c in "'`Â´â€™â€˜()+*->") and (i+1 == lentw) and (lentw > 1) and ((tw[i-1].isalpha()) or tw[i-1] in "0123456789"):
                newtw += ' '+c
            elif (c in ",") and ((i != 0) and (i+1 != lentw)) and tw[i-1].isdigit() and tw[i+1].isdigit(): # for 3,5 7,5
                newtw += c
            elif (c in ",&"):
                newtw += " "+c+" "
            elif (c in "Ã¢"):
                newtw += "a"
            elif (c in "Ãª"):
                newtw += "e"
            elif (c in "Ã®"):
                newtw += "i"
            elif (c in "Ãº"):
                newtw += "Ã¼"
            #elif (c in ":")
            else:
                newtw += c
            #print(c in "'`Â´â€™()+*-", lentw>i+1, i>0, tw[i-1] == ' 0123456789', tw[i+1].isalpha())
                
        return newtw

    def tokenize_df(self, tokdf,texcol="tweet", newtexcol='texttokCap',rescol="ttextlist", addLowerTok=True):
        #concert_table.drop_duplicates()
        # Note
        # tokdf[newtexcol] = tokdf[newtexcol].str.replace("""\xa0"""," ")
        # tokdf[newtexcol] = tokdf[newtexcol].str.replace("\n"," . ")
        
        tokdf[newtexcol] = tokdf[texcol].copy()
    
        tokdf[newtexcol] = tokdf[newtexcol].replace(self.toReplaceDict, regex=True)
        tokdf[newtexcol][tokdf[newtexcol].str.endswith(".")] = tokdf[tokdf[newtexcol].str.endswith(".")][newtexcol].apply(lambda tw: tw[:-1]+' .') 
        tokdf[newtexcol][tokdf[newtexcol].str.endswith(".'")] = tokdf[tokdf[newtexcol].str.endswith(".'")][newtexcol].apply(lambda tw: tw[:-2]+" . '") 
        tokdf[newtexcol][tokdf[newtexcol].str.startswith("'")] = tokdf[tokdf[newtexcol].str.startswith("'")][newtexcol].apply(lambda tw: "' "+tw[1:])
    
        tokdf[newtexcol] = tokdf[newtexcol].apply(self.tokenize)
        tokdf[newtexcol] = tokdf[newtexcol].str.strip()
        tokdf[rescol] = tokdf[newtexcol].str.split()
        
        if addLowerTok:
            tokdf[newtexcol[:-3]] = tokdf[newtexcol].str.lower()
    
        return tokdf.copy()
